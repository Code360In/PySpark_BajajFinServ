{"cells":[{"cell_type":"markdown","source":["## 03-pyspark-window-functions.py"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bf67b03-e94f-4067-9389-6b7c6736e749","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# 03-pyspark-window-functions.py \nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('PySparkExamples').getOrCreate()\n\nsimpleData = [(\"Jaya\", \"Sales\", 3000), (\"Mithun\", \"Sales\", 4600),\n              (\"Rohit\", \"Sales\", 4100), (\"Maya\", \"Finance\", 3000),\n              (\"Jaya\", \"Sales\", 3000), (\"Satish\", \"Finance\", 3300),\n              (\"Joy\", \"Finance\", 3900), (\"Jitendra\", \"Marketing\", 3000),\n              (\"Kumar\", \"Marketing\", 2000), (\"Sunu\", \"Sales\", 4100)]\n \ncolumns= [\"employee_name\", \"department\", \"salary\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e37ea49-6b40-4964-9bcb-7c0da4b08d69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Jaya         |Sales     |3000  |\n|Mithun       |Sales     |4600  |\n|Rohit        |Sales     |4100  |\n|Maya         |Finance   |3000  |\n|Jaya         |Sales     |3000  |\n|Satish       |Finance   |3300  |\n|Joy          |Finance   |3900  |\n|Jitendra     |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Sunu         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Jaya         |Sales     |3000  |\n|Mithun       |Sales     |4600  |\n|Rohit        |Sales     |4100  |\n|Maya         |Finance   |3000  |\n|Jaya         |Sales     |3000  |\n|Satish       |Finance   |3300  |\n|Joy          |Finance   |3900  |\n|Jitendra     |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Sunu         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e713c55b-c3db-4c08-87ef-5a883e0e403d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ROW_NUMBER() function is a SQL ranking function that assigns a sequential rank number to each new record in a partition. When the SQL Server ROW NUMBER function detects two identical values in the same partition, it assigns different rank numbers to both. The rank number will be determined by the sequence in which they are displayed.\n\ndf.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n    .show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88839bfa-c176-43ed-b1c8-e3d443b0f8ad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maya         |Finance   |3000  |1         |\n|Satish       |Finance   |3300  |2         |\n|Joy          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jitendra     |Marketing |3000  |2         |\n|Jaya         |Sales     |3000  |1         |\n|Jaya         |Sales     |3000  |2         |\n|Rohit        |Sales     |4100  |3         |\n|Sunu         |Sales     |4100  |4         |\n|Mithun       |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maya         |Finance   |3000  |1         |\n|Satish       |Finance   |3300  |2         |\n|Joy          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jitendra     |Marketing |3000  |2         |\n|Jaya         |Sales     |3000  |1         |\n|Jaya         |Sales     |3000  |2         |\n|Rohit        |Sales     |4100  |3         |\n|Sunu         |Sales     |4100  |4         |\n|Mithun       |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# RANK() function is a window function could be used in SQL Server to calculate a rank for each row within a partition of a result set. The same rank is assigned to the rows in a partition which have the same values. The rank of the first row is 1.\n# The ranks may not be consecutive in the RANK() function as it adds the number of repeated rows to the repeated rank to calculate the rank of the next row.\n\nfrom pyspark.sql.functions import rank\ndf.withColumn(\"rank\", rank().over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"713e11ee-0cf5-48de-aa0a-b2aac3a39580","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|         Maya|   Finance|  3000|   1|\n|       Satish|   Finance|  3300|   2|\n|          Joy|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|     Jitendra| Marketing|  3000|   2|\n|         Jaya|     Sales|  3000|   1|\n|         Jaya|     Sales|  3000|   1|\n|        Rohit|     Sales|  4100|   3|\n|         Sunu|     Sales|  4100|   3|\n|       Mithun|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|         Maya|   Finance|  3000|   1|\n|       Satish|   Finance|  3300|   2|\n|          Joy|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|     Jitendra| Marketing|  3000|   2|\n|         Jaya|     Sales|  3000|   1|\n|         Jaya|     Sales|  3000|   1|\n|        Rohit|     Sales|  4100|   3|\n|         Sunu|     Sales|  4100|   3|\n|       Mithun|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# DENSE_RANK() function assigns a rank to each row within each group of rows with the same name value, based on the following rules:\n# 1. Rows with the highest score values are assigned the rank of 1.\n# 2. Rows with the next highest score values are assigned the rank of 2, and so on.\n# 3. Rows with the same score value are assigned the same rank.\n\nfrom pyspark.sql.functions import dense_rank\ndf.withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57212574-ed68-47bf-a346-cb60fda3b391","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|         Maya|   Finance|  3000|         1|\n|       Satish|   Finance|  3300|         2|\n|          Joy|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|     Jitendra| Marketing|  3000|         2|\n|         Jaya|     Sales|  3000|         1|\n|         Jaya|     Sales|  3000|         1|\n|        Rohit|     Sales|  4100|         2|\n|         Sunu|     Sales|  4100|         2|\n|       Mithun|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|         Maya|   Finance|  3000|         1|\n|       Satish|   Finance|  3300|         2|\n|          Joy|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|     Jitendra| Marketing|  3000|         2|\n|         Jaya|     Sales|  3000|         1|\n|         Jaya|     Sales|  3000|         1|\n|        Rohit|     Sales|  4100|         2|\n|         Sunu|     Sales|  4100|         2|\n|       Mithun|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# PERCENT_RANK() is a window function that calculates the percentile ranking of rows in a result set. The PERCENT_RANK() function returns a percentile ranking number which ranges from zero to one. For a specific row, PERCENT_RANK() uses the following formula to calculate the percentile rank:\n# (rank - 1) / (total_rows - 1)\n\nfrom pyspark.sql.functions import percent_rank\ndf.withColumn(\"percent_rank\", percent_rank().over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85eefc9f-2262-4b01-9512-40abc3f15443","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------+\n|employee_name|department|salary|percent_rank|\n+-------------+----------+------+------------+\n|         Maya|   Finance|  3000|         0.0|\n|       Satish|   Finance|  3300|         0.5|\n|          Joy|   Finance|  3900|         1.0|\n|        Kumar| Marketing|  2000|         0.0|\n|     Jitendra| Marketing|  3000|         1.0|\n|         Jaya|     Sales|  3000|         0.0|\n|         Jaya|     Sales|  3000|         0.0|\n|        Rohit|     Sales|  4100|         0.5|\n|         Sunu|     Sales|  4100|         0.5|\n|       Mithun|     Sales|  4600|         1.0|\n+-------------+----------+------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------+\n|employee_name|department|salary|percent_rank|\n+-------------+----------+------+------------+\n|         Maya|   Finance|  3000|         0.0|\n|       Satish|   Finance|  3300|         0.5|\n|          Joy|   Finance|  3900|         1.0|\n|        Kumar| Marketing|  2000|         0.0|\n|     Jitendra| Marketing|  3000|         1.0|\n|         Jaya|     Sales|  3000|         0.0|\n|         Jaya|     Sales|  3000|         0.0|\n|        Rohit|     Sales|  4100|         0.5|\n|         Sunu|     Sales|  4100|         0.5|\n|       Mithun|     Sales|  4600|         1.0|\n+-------------+----------+------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# NTILE() function is a window function that distributes rows of an ordered partition into a pre-defined number of roughly equal groups. It assigns each group a number_expression ranging from 1.\n\nfrom pyspark.sql.functions import ntile\ndf.withColumn(\"ntile\", ntile(2).over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed38ffad-ec0f-475c-be87-9dfa2a32a6a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+-----+\n|employee_name|department|salary|ntile|\n+-------------+----------+------+-----+\n|         Maya|   Finance|  3000|    1|\n|       Satish|   Finance|  3300|    1|\n|          Joy|   Finance|  3900|    2|\n|        Kumar| Marketing|  2000|    1|\n|     Jitendra| Marketing|  3000|    2|\n|         Jaya|     Sales|  3000|    1|\n|         Jaya|     Sales|  3000|    1|\n|        Rohit|     Sales|  4100|    1|\n|         Sunu|     Sales|  4100|    2|\n|       Mithun|     Sales|  4600|    2|\n+-------------+----------+------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+-----+\n|employee_name|department|salary|ntile|\n+-------------+----------+------+-----+\n|         Maya|   Finance|  3000|    1|\n|       Satish|   Finance|  3300|    1|\n|          Joy|   Finance|  3900|    2|\n|        Kumar| Marketing|  2000|    1|\n|     Jitendra| Marketing|  3000|    2|\n|         Jaya|     Sales|  3000|    1|\n|         Jaya|     Sales|  3000|    1|\n|        Rohit|     Sales|  4100|    1|\n|         Sunu|     Sales|  4100|    2|\n|       Mithun|     Sales|  4600|    2|\n+-------------+----------+------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# CUME_DIST() is a window function that calculates the cumulative distribution of value within a set of values. The CUME_DIST() function returns a value that represents the number of rows with values less than or equal to (<= )the current row’s value divided by the total number of rows: i.e. N / total_rows\n# where N is the number of rows with the value less than or equal to the current row value.\n# and total_rows is the number of rows in the partition or result set being evaluated.\n\nfrom pyspark.sql.functions import cume_dist    \ndf.withColumn(\"cume_dist\", cume_dist().over(windowSpec)) \\\n   .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b9d1c86-86fa-488c-98fb-a6efb3004610","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------------+\n|employee_name|department|salary|         cume_dist|\n+-------------+----------+------+------------------+\n|         Maya|   Finance|  3000|0.3333333333333333|\n|       Satish|   Finance|  3300|0.6666666666666666|\n|          Joy|   Finance|  3900|               1.0|\n|        Kumar| Marketing|  2000|               0.5|\n|     Jitendra| Marketing|  3000|               1.0|\n|         Jaya|     Sales|  3000|               0.4|\n|         Jaya|     Sales|  3000|               0.4|\n|        Rohit|     Sales|  4100|               0.8|\n|         Sunu|     Sales|  4100|               0.8|\n|       Mithun|     Sales|  4600|               1.0|\n+-------------+----------+------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------------+\n|employee_name|department|salary|         cume_dist|\n+-------------+----------+------+------------------+\n|         Maya|   Finance|  3000|0.3333333333333333|\n|       Satish|   Finance|  3300|0.6666666666666666|\n|          Joy|   Finance|  3900|               1.0|\n|        Kumar| Marketing|  2000|               0.5|\n|     Jitendra| Marketing|  3000|               1.0|\n|         Jaya|     Sales|  3000|               0.4|\n|         Jaya|     Sales|  3000|               0.4|\n|        Rohit|     Sales|  4100|               0.8|\n|         Sunu|     Sales|  4100|               0.8|\n|       Mithun|     Sales|  4600|               1.0|\n+-------------+----------+------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# LAG() is a window function that provides access to a row at a specified physical offset which comes before the current row.\n\nfrom pyspark.sql.functions import lag    \ndf.withColumn(\"lag\", lag(\"salary\", 2).over(windowSpec)) \\\n      .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c6b75ca-3f2f-45e0-a3ee-83f99da03d02","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|         Maya|   Finance|  3000|null|\n|       Satish|   Finance|  3300|null|\n|          Joy|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|     Jitendra| Marketing|  3000|null|\n|         Jaya|     Sales|  3000|null|\n|         Jaya|     Sales|  3000|null|\n|        Rohit|     Sales|  4100|3000|\n|         Sunu|     Sales|  4100|3000|\n|       Mithun|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|         Maya|   Finance|  3000|null|\n|       Satish|   Finance|  3300|null|\n|          Joy|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|     Jitendra| Marketing|  3000|null|\n|         Jaya|     Sales|  3000|null|\n|         Jaya|     Sales|  3000|null|\n|        Rohit|     Sales|  4100|3000|\n|         Sunu|     Sales|  4100|3000|\n|       Mithun|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# LEAD() is an analytic function that lets you query more than one row in a table at a time without having to join the table to itself. It returns values from the next row in the table.\n\nfrom pyspark.sql.functions import lead    \ndf.withColumn(\"lead\", lead(\"salary\", 2).over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc7fb4bf-450e-4505-9ac4-8478df93a3be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|         Maya|   Finance|  3000|3900|\n|       Satish|   Finance|  3300|null|\n|          Joy|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|     Jitendra| Marketing|  3000|null|\n|         Jaya|     Sales|  3000|4100|\n|         Jaya|     Sales|  3000|4100|\n|        Rohit|     Sales|  4100|4600|\n|         Sunu|     Sales|  4100|null|\n|       Mithun|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|         Maya|   Finance|  3000|3900|\n|       Satish|   Finance|  3300|null|\n|          Joy|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|     Jitendra| Marketing|  3000|null|\n|         Jaya|     Sales|  3000|4100|\n|         Jaya|     Sales|  3000|4100|\n|        Rohit|     Sales|  4100|4600|\n|         Sunu|     Sales|  4100|null|\n|       Mithun|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["windowSpecAgg = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col, avg, sum, min, max, row_number \n\ndf.withColumn(\"row\", row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\") == 1).select(\"department\", \"avg\", \"sum\", \"min\", \"max\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67554087-794e-49ec-83e1-1dfe07d1b8ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03-pyspark-window-functions.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1674564741910371}},"nbformat":4,"nbformat_minor":0}
