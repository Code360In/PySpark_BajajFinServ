{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca22649e-72d6-4334-a950-941dcb0c9804",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 04-pyspark-rdd-actions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30bbb8cd-7d53-44d6-95f7-3ab474bf9f5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300 400\n",
       "20 <class 'int'>\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "300 400\n20 <class 'int'>\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 04-pyspark-rdd-actions.py\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PySparkExamples').getOrCreate()\n",
    "data = [(\"Z\", 1), (\"A\", 20), (\"B\", 30), (\"C\", 40), (\"B\", 30), (\"B\", 60)]\n",
    "\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "listRDD = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 3, 2])\n",
    "\n",
    "# aggregate\n",
    "seqOp = (lambda x, y: x + y)  # elementwise sequential operation function\n",
    "combOp = (lambda x, y: x + y)  # tuplewise operation function\n",
    "\n",
    "print (seqOp(100, 200), combOp(100, 300))\n",
    "agg1 = listRDD.aggregate(0, seqOp, combOp)\n",
    "print(agg1, type(agg1)) # output 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f036569-16fc-4728-965c-98df7b3021f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 7)\n",
       "20 <class 'int'>\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "(20, 7)\n20 <class 'int'>\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# aggregate 2\n",
    "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "agg2 = listRDD.aggregate((0, 0), seqOp2, combOp2)\n",
    "print(agg2) # output (20, 7)\n",
    "\n",
    "agg2 = listRDD.treeAggregate(0, seqOp, combOp)\n",
    "print(agg2, type(agg2)) # output 20\n",
    "# check the detailed calculation here: \n",
    "# https://stackoverflow.com/questions/43270798/how-does-rdd-aggregate-action-work-i-e-how-to-understand-the-parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# aggregate 1\n",
    "seqOp  = (lambda x, y: x + y)  # elementwise sequential operation function\n",
    "combOp = (lambda x, y: x + y)  # tuplewise operation function\n",
    "\n",
    "listRDD = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 3, 2])\n",
    "agg1 = listRDD.aggregate(0, seqOp, combOp)\n",
    "print(agg1, type(agg1))        # output 20\n",
    "\n",
    "Current x    x     y\n",
    "        0    0+1   1\n",
    "        1    1+2   2\n",
    "        3    3+3   3\n",
    "        6    6+4   4\n",
    "       10    10+5  5\n",
    "       15    15+3  3\n",
    "       18    18+2  2\n",
    "       20\n",
    "\n",
    "# aggregate 2\n",
    "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))         # elementwise sequential operation function\n",
    "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))  # tuplewise operation function\n",
    "\n",
    "listRDD = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 3, 2])\n",
    "agg2 = listRDD.aggregate((0, 0), seqOp2, combOp2)\n",
    "print(agg2)                                          # output (20, 7)\n",
    "\n",
    "[(1, 2), (3, 4), (5, 3), 2]\n",
    "y = (1, 2)\n",
    "Current x    x[0]    x[1]    y\n",
    "   (0, 0)    0+1     0+1     1\n",
    "   (1, 1)    1+2     1+1     2\n",
    "   (3, 2)\n",
    "\n",
    "y = (3, 4)\n",
    "Current x    x[0]    x[1]    y\n",
    "   (0, 0)    0+3     0+1     3\n",
    "   (3, 1)    3+4     1+1     4\n",
    "   (7, 2)\n",
    "\n",
    "y = (5, 3)\n",
    "Current x    x[0]    x[1]    y\n",
    "   (0, 0)    0+5     0+1     5\n",
    "   (5, 1)    5+3     1+1     3\n",
    "   (8, 2)\n",
    "\n",
    "y = 2\n",
    "Current x    x[0]    x[1]    y\n",
    "   (0, 0)    0+2     0+1     2\n",
    "   (2, 1)\n",
    "\n",
    "Current x    x[0]    x[1]    y[0]    y[1]\n",
    "   (0, 0)    0+3     0+2     3       2\n",
    "   (3, 2)    3+7     2+2     7       3    \n",
    "   (10, 4)   10+8    4+2     8       2\n",
    "   (18, 6)   18+2    6+1     2       1\n",
    "   (20, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b060374-ae02-4cd8-a821-2b32752c5da6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20\n",
       "20\n",
       "20\n",
       "[1, 2, 3, 4, 5, 3, 2]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "20\n20\n20\n[1, 2, 3, 4, 5, 3, 2]\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fold\n",
    "from operator import add\n",
    "foldRes = listRDD.fold(0, add)\n",
    "print(foldRes) # output 20\n",
    "\n",
    "# reduce\n",
    "redRes = listRDD.reduce(add)\n",
    "print(redRes) # output 20\n",
    "\n",
    "# treeReduce. This is similar to reduce\n",
    "add = lambda x, y: x + y\n",
    "redRes = listRDD.treeReduce(add)\n",
    "print(redRes) # output 20\n",
    "\n",
    "# Collect\n",
    "data = listRDD.collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2709be43-f3e1-4e13-aec9-09b11950dceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Count: 7\n",
       "countApprox: 7\n",
       "countApproxDistinct: 5\n",
       "countApproxDistinct: 5\n",
       "countByValue: defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Count: 7\ncountApprox: 7\ncountApproxDistinct: 5\ncountApproxDistinct: 5\ncountByValue: defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count, countApprox, countApproxDistinct\n",
    "print(\"Count: \" + str(listRDD.count()))\n",
    "print(\"countApprox: \" + str(listRDD.countApprox(1200)))\n",
    "print(\"countApproxDistinct: \" + str(listRDD.countApproxDistinct()))\n",
    "print(\"countApproxDistinct: \" + str(inputRDD.countApproxDistinct()))\n",
    "\n",
    "# countByValue\n",
    "print(\"countByValue: \" + str(listRDD.countByValue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd273c22-6448-4be7-bfd9-c9f665d29ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first: 1\n",
       "first:  ('Z', 1)\n",
       "top: [5, 4]\n",
       "top: [('Z', 1), ('C', 40)]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "first: 1\nfirst:  ('Z', 1)\ntop: [5, 4]\ntop: [('Z', 1), ('C', 40)]\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first\n",
    "print(\"first: \" + str(listRDD.first()))\n",
    "print(\"first:  \" + str(inputRDD.first()))\n",
    "\n",
    "# top\n",
    "print(\"top: \" + str(listRDD.top(2)))\n",
    "print(\"top: \" + str(inputRDD.top(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b88221a5-b8ff-4faa-8c1c-c6b3d2673ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min: 1\n",
       "min: ('A', 20)\n",
       "max: 5\n",
       "max: ('Z', 1)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "min: 1\nmin: ('A', 20)\nmax: 5\nmax: ('Z', 1)\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# min\n",
    "print(\"min: \" + str(listRDD.min()))\n",
    "print(\"min: \" + str(inputRDD.min()))\n",
    "\n",
    "# max\n",
    "print(\"max: \" + str(listRDD.max()))\n",
    "print(\"max: \" + str(inputRDD.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8a6059-4d86-4ff2-9851-58bfbcb2896c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "take: [1, 2]\n",
       "takeOrdered: [1, 2]\n",
       "take: [3, 2]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "take: [1, 2]\ntakeOrdered: [1, 2]\ntake: [3, 2]\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take, takeOrdered, takeSample\n",
    "print(\"take: \" + str(listRDD.take(2)))\n",
    "# Output: take : 1,2\n",
    "print(\"takeOrdered: \" + str(listRDD.takeOrdered(2)))\n",
    "# Output: takeOrdered : 1,2\n",
    "print(\"take: \" + str(listRDD.takeSample(10, 2)))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-pyspark-rdd-actions.py",
   "notebookOrigID": 1830562702316104,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
